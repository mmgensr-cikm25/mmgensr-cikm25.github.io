<section id="call" class={{include.class}}>
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading">Call for Papers</h2>
        <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
      </div>
    </div>
    <div class="row text-justify">
      <div class="col-md-12">
        <p class="large text-muted">
            The main objective of this workshop is to encourage pioneering research at the intersection of generative models with multimodal search and recommendation. The overarching theme is the leveraging of generative AI to enhance and revolutionize information access and personalized content delivery in these multimodal scenarios. This workshop aims to attract a diverse audience, including academic researchers and industry experts working on or interested in generative models, multimodal information retrieval, and recommender systems. It offers a unique forum for these stakeholders to share innovative ideas, methods, and accomplishments, encouraging interdisciplinary collaboration and the exploration of novel applications. Specifically, we invite contributions addressing three key areas: (1) Generative Retrieval and Recommendation utilizing Large Multimodal Models (LMMs) and Multimodal Large Language Models (MLLMs), (2) Advanced Content Generation methodologies within Generative Search and Recommendation systems, and (3) Domain-specific Applications, Benchmarks, and Deployment strategies.      
        </p>
        <p class="large text-muted">
            Topics of interest include, but are not limited to:        </p>  
        <ul class="large text-muted">
          <li><strong>LMM/MLLM for generative retrieval & recommendation</strong>
            <ul>
              <li>Vision-language models for personalized recommendation.</li>
              <li>Multimodal representation learning for generative search.</li>
              <li>Cross-modal and modality-enhanced zero-shot recommendation systems.</li>
              <li>Reasoning and explainability in multimodal generative search systems.</li>
              <li>Multimodal dense/sparse retrieval methods.</li>
              <li>Large foundation models and large-scale benchmark datasets for multimodal search/recommendation.</li>
              <li>Multimodal memory-augmented models and long-term personalization.</li>
              <li>Trustworthiness and efficiency in multimodal search and recommendation.</li>
              <li>Privacy-preserving personalization in multimodal generative recommendation.</li>
            </ul>
          </li>
        
          <li><strong>Content generation in generative search & recommendation</strong>
            <ul>
              <li>Generative models (e.g., large language models, diffusion models) for recommendation tasks.</li>
              <li>Image, video, and text generation for product or content search.</li>
              <li>Human-AI co-creation in multimodal content search.</li>
              <li>Privacy-preserving multimodal content generation.</li>
              <li>Evaluation protocols for hallucination and factuality in multimodal content generation.</li>
            </ul>
          </li>
        
          <li><strong>Vertical applications, benchmarks & deployment</strong>
            <ul>
              <li>Fashion, food, and lifestyle recommendation with multimodal inputs.</li>
              <li>Generative recommendation in e-commerce and retail scenarios.</li>
              <li>Domain-specific benchmarks for multimodal search and recommendation.</li>
              <li>Generating comprehensive reports for specific tasks using multimodal retrieval and generation.</li>
              <li>Scalable system architectures and deployment frameworks for real-world generative multimodal search and recommendation.</li>
            </ul>
          </li>
        </ul>
        <p class="large text-muted">
          <strong>Submission Guidelines: Authors are invited to submit original, full-length research papers</strong> that are not previously published, accepted to be published, or being considered for publication in any other forum. Manuscripts should be submitted to the <strong>CIKM 2025 EasyChair site</strong> in <strong>PDF format</strong>, using the <strong>2-column <em>sigconf</em> format</strong> from the <a href="https://www.acm.org/publications/proceedings-template" target="_blank">ACM proceedings template</a>. Submissions can be of varying length from <strong>4 to 9 pages</strong>, plus <strong>unlimited pages for references</strong>. The authors may decide on the appropriate length of the paper as no distinction is made between long and short papers. The review process will be <strong>double-blind</strong>, and submissions that are not properly anonymized will be <strong>desk-rejected</strong> without review. <strong> Each submission will be allocated to a minimum of three program committee members without COI for review. After collecting reviewersâ€™ comments, a meeting will be scheduled by the organizers to discuss reviews and make the final decision. At least one author of each accepted paper must register and present the work on-site in Seoul, Korea</strong>, as scheduled in the official CIKM 2025 conference program.
        </p>

        <p class="large text-muted">
          <strong>In addition, we also invite a selection of outstanding papers recently accepted or published in top-tier conferences and journals, to participate in the discussion on the latest research advances (Please contact yi.bin@hotmail.com.</strong>
        </p>
        
        <p class="large text-muted">
          <strong>Submission site: <a href="https://easychair.org/conferences?conf=mmgensr2025" target="_blank">https://easychair.org/conferences?conf=mmgensr2025</a></strong>
        </p>         

      </div>
    </div>
  </div>
</section>
