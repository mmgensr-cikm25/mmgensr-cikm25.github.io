<section id="call" class={{include.class}}>
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading">Call for Papers</h2>
        <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
      </div>
    </div>
    <div class="row text-justify">
      <div class="col-md-12">
        <p class="large text-muted">
            This workshop aims to foster pioneering research by integrating powerful generative models into both multimodal search and recommender systems. Its objectives are to convene experts for discussing innovative concepts, architectures, methodologies, and evaluation protocols in multimodal generative search and recommendation, while analyzing complexities in areas like personalized content generation, user interaction, and trustworthiness. The primary goals include deepening the understanding of how LLMs/MLLMs enhance these tasks, discussing effective data modality integration, and addressing challenges such as semantic alignment, bias, and efficiency. Expected outcomes encompass a clearer articulation of the state-of-the-art, generation of new research ideas and collaborations, increased community awareness, and contributions that will spur innovation in academia and industry.       
        </p>
        <p class="large text-muted">
            The workshop provides an invaluable forum for researchers to present the latest advancements in the rapidly evolving area of multimodal generative search and recommendation. We welcome original submissions focusing on multimodal generative models for search and recommendation, including a range of relevant topics:        </p>  
        <ul class="large text-muted">
          <li><strong>LMM/MLLM for generative retrieval & recommendation</strong>
            <ul>
              <li>Vision-language models for personalized recommendation.</li>
              <li>Multimodal representation learning for generative search.</li>
              <li>Cross-modal and modality-enhanced zero-shot recommendation systems.</li>
              <li>Reasoning and explainability in multimodal generative search systems.</li>
              <li>Multimodal dense/sparse retrieval methods.</li>
              <li>Large foundation models and large-scale benchmark datasets for multimodal search/recommendation.</li>
              <li>Multimodal memory-augmented models and long-term personalization.</li>
              <li>Trustworthiness and efficiency in multimodal search and recommendation.</li>
              <li>Privacy-preserving personalization in multimodal generative recommendation.</li>
            </ul>
          </li>
        
          <li><strong>Content generation in generative search & recommendation</strong>
            <ul>
              <li>Generative models (e.g., large language models, diffusion models) for recommendation tasks.</li>
              <li>Image, video, and text generation for product or content search.</li>
              <li>Human-AI co-creation in multimodal content search.</li>
              <li>Privacy-preserving multimodal content generation.</li>
              <li>Evaluation protocols for hallucination and factuality in multimodal content generation.</li>
            </ul>
          </li>
        
          <li><strong>Vertical applications, benchmarks & deployment</strong>
            <ul>
              <li>Fashion, food, and lifestyle recommendation with multimodal inputs.</li>
              <li>Generative recommendation in e-commerce and retail scenarios.</li>
              <li>Domain-specific benchmarks for multimodal search and recommendation.</li>
              <li>Generating comprehensive reports for specific tasks using multimodal retrieval and generation.</li>
            </ul>
          </li>
        </ul>
        <p class="large text-muted">
          <strong>Submission Guidelines: Authors are invited to submit original, full-length research papers</strong> that are not previously published, accepted to be published, or being considered for publication in any other forum. Manuscripts should be submitted to the <strong>CIKM 2025 EasyChair site</strong> in <strong>PDF format</strong>, using the <strong>2-column <em>sigconf</em> format</strong> from the <a href="https://www.acm.org/publications/proceedings-template" target="_blank">ACM proceedings template</a>. Submissions can be of varying length from <strong>4 to 9 pages</strong>, plus <strong>unlimited pages for references</strong>. The authors may decide on the appropriate length of the paper as no distinction is made between long and short papers. The review process will be <strong>double-blind</strong>, and submissions that are not properly anonymized will be <strong>desk-rejected</strong> without review. <strong>At least one author of each accepted paper must register and present the work on-site in Seoul, Korea</strong>, as scheduled in the official CIKM 2025 conference program.
        </p>
        
<!-- Submission site: <a href="https://easychair.org/conferences/?conf=genrec23">https://easychair.org/conferences/?conf=genrec23</a>.  -->
         

      </div>
    </div>
  </div>
</section>
